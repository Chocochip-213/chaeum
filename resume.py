# -*- coding: utf-8 -*-
"""Resume.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JvGBdVJ9ADGL2oPy8O9iUPMxVRTLdH9Z
"""

# 2. 호환성 수정된 버전 설치 (핵심: bitsandbytes를 0.45.0 이상으로 올림)
# Python 3.12 및 최신 Triton 지원은 0.45.0 버전부터 안정화되었습니다.
!pip install -q -U "bitsandbytes>=0.45.0"
!pip install -q -U git+https://github.com/huggingface/transformers.git
!pip install -q -U peft accelerate trl datasets

# 3. Triton 재설치 (최신 BnB와 호환되는 버전)
!pip install -q -U triton

print("재설치 완료. 상단 메뉴 [런타임] -> [세션 다시 시작] 후 import를 진행하세요.")

import torch
import bitsandbytes as bnb

# GPU 인식 확인
print(f"CUDA Available: {torch.cuda.is_available()}")
print(f"Bitsandbytes Version: {bnb.__version__}")

# 여기서 에러가 안 나야 모델 로드가 가능합니다.
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

import os
import torch
from google.colab import drive
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments
)
from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model
from trl import SFTTrainer, SFTConfig  # 이제 에러 안 날 겁니다.

print(f"TRL 및 라이브러리 로드 성공!")

# 1. 고속 다운로드 라이브러리 설치
!pip install -q hf_transfer

# 2. 환경 변수 설정 (이게 핵심!)
import os
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"

print("✅ 초고속 병렬 다운로드(hf_transfer) 모드가 활성화되었습니다.")

# 2. GPU 환경 감지 (자동 설정)
# T4(무료) vs A100/L4(유료)에 따라 최적화 옵션 자동 변경
gpu_name = torch.cuda.get_device_name(0)
is_high_end_gpu = any(x in gpu_name for x in ["A100", "L4", "3090", "4090"])
print(f"현재 GPU: {gpu_name} (고성능 모드: {is_high_end_gpu})")

# 3. 경로 및 파라미터 설정
DATA_PATH = "/content/drive/MyDrive/resumeAnalysis/data/merged_dataset.jsonl"
OUTPUT_DIR = "/content/drive/MyDrive/Qwen2.5-14B-Resume-Analysis-Checkpoints"
MODEL_ID = "Qwen/Qwen2.5-14B-Instruct"

# 4. 데이터셋 로드 및 전처리 (Train 9 : Val 1 분리)
full_dataset = load_dataset("json", data_files=DATA_PATH, split="train")

# Qwen 채팅 템플릿 적용을 위해 컬럼명 변경 (conversations -> messages)
if "conversations" in full_dataset.column_names:
    full_dataset = full_dataset.rename_column("conversations", "messages")

# 9:1 비율로 분리
dataset_split = full_dataset.train_test_split(test_size=0.1, seed=42)
train_dataset = dataset_split["train"]
eval_dataset = dataset_split["test"]

print(f"✅ 데이터 준비 완료 | 학습용: {len(train_dataset)}개, 검증용: {len(eval_dataset)}개")
print("데이터 예시:", train_dataset[0])

# 1. 4-bit 양자화 설정 (메모리 절약 핵심)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16 if is_high_end_gpu else torch.float16,
    bnb_4bit_use_double_quant=True,
)

# 2. 토크나이저 로드
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

# 3. 모델 로드
print("모델을 로드하는 중입니다... (시간이 조금 걸립니다)")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    attn_implementation="eager"  # A100에서는 강제로 켜기
)

# 4. 학습 준비 (Gradient Checkpointing 활성화)
model = prepare_model_for_kbit_training(model)
model.gradient_checkpointing_enable() # 메모리 폭발 방지 필수 설정

# 5. LoRA 어댑터 설정
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)
print("✅ 모델 및 LoRA 설정 완료")

from transformers import TrainingArguments

# =================================================================
# 1. 포맷팅 함수 (이건 잘 작동했습니다)
# =================================================================
def formatting_prompts_func(example):
    output_texts = []
    for message in example['messages']:
        text = tokenizer.apply_chat_template(
            message,
            tokenize=False,
            add_generation_prompt=False
        )
        output_texts.append(text)
    return output_texts

# =================================================================
# 2. 학습 설정 (수정됨)
# =================================================================
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=3,

    # A100 최적화
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    gradient_accumulation_steps=4,

    learning_rate=2e-4,
    logging_steps=10,
    eval_strategy="steps",
    eval_steps=50,
    save_strategy="steps",
    save_steps=50,
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",

    fp16=False,
    bf16=True, # A100

    report_to="none",
    gradient_checkpointing=True,
    gradient_checkpointing_kwargs={"use_reentrant": False},

    # 🔥 [수정 완료] False -> True로 변경 (또는 이 줄 삭제)
    # True로 해야 'messages' 같은 원본 컬럼을 버리고 토큰화된 데이터만 모델에 넣습니다.
    remove_unused_columns=True
)

# =================================================================
# 3. Trainer 생성
# =================================================================
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    peft_config=peft_config,
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=False,

    # 포맷팅 함수 연결
    formatting_func=formatting_prompts_func,
)

# =================================================================
# 4. 이어하기 및 학습 시작
# =================================================================
last_checkpoint = None
if os.path.isdir(OUTPUT_DIR):
    checkpoints = [d for d in os.listdir(OUTPUT_DIR) if d.startswith("checkpoint-")]
    if checkpoints:
        checkpoints.sort(key=lambda x: int(x.split("-")[1]))
        last_checkpoint = os.path.join(OUTPUT_DIR, checkpoints[-1])
        print(f"!!! 기존 체크포인트 발견: {last_checkpoint} 에서 학습을 재개합니다.")

print("🚀 A100 환경에서 학습을 시작합니다...")
trainer.train(resume_from_checkpoint=last_checkpoint)

# 5. 최종 저장
FINAL_SAVE_PATH = os.path.join(OUTPUT_DIR, "final_adapter")
trainer.save_model(FINAL_SAVE_PATH)
tokenizer.save_pretrained(FINAL_SAVE_PATH)
print(f"✅ 학습 완료! 최종 모델 저장 경로: {FINAL_SAVE_PATH}")

import matplotlib.pyplot as plt

# Trainer 로그 기록 가져오기
history = trainer.state.log_history

# 데이터 추출
train_loss = []
train_steps = []
eval_loss = []
eval_steps = []

for entry in history:
    if 'loss' in entry:
        train_loss.append(entry['loss'])
        train_steps.append(entry['step'])
    elif 'eval_loss' in entry:
        eval_loss.append(entry['eval_loss'])
        eval_steps.append(entry['step'])

# 그래프 그리기
plt.figure(figsize=(12, 6))
plt.plot(train_steps, train_loss, label='Training Loss', alpha=0.6)
plt.plot(eval_steps, eval_loss, label='Validation Loss', color='red', marker='o')
plt.title('Training & Validation Loss Curve')
plt.xlabel('Steps')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

"""# 아래는 추론

"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer
from peft import PeftModel

# ==========================================
# 1. 모델 및 어댑터 로드 설정
# ==========================================
BASE_MODEL_ID = "Qwen/Qwen2.5-14B-Instruct"
ADAPTER_PATH = "/content/drive/MyDrive/Qwen2.5-14B-Resume-Analysis-Checkpoints/final_adapter"

print("🚀 모델 로드 중... (잠시만 기다려주세요)")

# 4-bit 양자화 설정
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

# 베이스 모델 로드
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

# 학습된 LoRA 어댑터 결합
model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)
model.eval()

# 토크나이저 로드
try:
    tokenizer = AutoTokenizer.from_pretrained(ADAPTER_PATH, trust_remote_code=True)
except:
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

# [추가됨] 스트리머 설정: 답변만 깔끔하게 출력하기 위해 skip_prompt=True 설정
streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

print("✅ 모델 로드 완료!")

# ==========================================
# 2. 추론 함수 정의 (Streamer 적용)
# ==========================================
def analyze_resume(resume_text, jd_text):
    system_prompt = (
        "당신은 전문 테크니컬 리크루터이자 커리어 코치입니다.\n"
        "지원자의 이력서를 분석하여 보유 기술(Extracted Skills)을 파악하고, "
        "직무 기술서(JD)와의 격차(Gap)를 분석하는 것이 당신의 임무입니다.\n"
        "부족한 기술(Missing Skills)을 식별하고, 왜 그것이 부족한지 상세한 이유를 한국어로 설명하십시오."
        "missing_skills_chapters를 포함하여 JSON을 생성하라"
    )

    user_prompt = f"""
### Resume:
{resume_text}

### Job Description:
{jd_text}

Please analyze the resume skills and perform a Gap Analysis.
"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    model_inputs = tokenizer([text], return_tensors="pt").to("cuda")

    print("\n" + "="*30)
    print("🤖 분석 시작 (실시간 생성 중...)")
    print("="*30 + "\n")

    # 생성 (Inference)
    with torch.no_grad():
        generated_ids = model.generate(
            **model_inputs,
            streamer=streamer,      # [핵심] 여기에 스트리머 추가! 이제 화면에 바로 나옵니다.
            max_new_tokens=4096,    # 사용자님 설정 유지 (긴 분석을 위해 필수)
            temperature=0.1,        # 사용자님 설정 유지 (정확한 분석을 위해 필수)
            do_sample=True,
            top_p=0.9,
            repetition_penalty=1.05
        )

    # 텍스트 반환을 위한 디코딩 (화면 출력과는 별개로 변수에 저장하고 싶을 때 사용)
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

    return response

# ==========================================
# 3. 테스트 데이터셋 정의 (사용자 제공 데이터)
# ==========================================

# CASE 1: 합성 데이터 (김수진)
case1_resume = '''
- 이름: 김수진
- 생년월일 (만 26세)
- 전화번호: 010-****-1234
- 이메일: kimsoo123@xxmail.com

**학력:**
- OO초등학교 졸업
- OO중학교 졸업
- OO고등학교 졸업
- XX대학교 경영학과 (졸업)

**경력 및 경험:**
- 2022.03 - 2022.12: A문화센터, IT 지원 인턴
  - 외부 서버 관리 및 유지보수 지원
  - 기본적인 컴퓨터 문제 해결 및 사용자 지원 경험

**기술 및 역량:**
- Node.js 및 Express 사용 경험 없음
- MongoDB에 대한 기본적인 정보만 이해
- 데이터 구조 및 알고리즘에 대한 이해 부족
- Git 사용 경험 있으나 실제 프로젝트에 활용한 경험 없음

**기타:**
- 정보처리 기능사 자격증 취득
- IT 관련 비영리 단체에서 봉사활동 경험 (웹사이트 잘못된 링크 수정 등)
- 관련 프로젝트 경험 없음
'''

case1_jd = '''
**기업명**: [OO인터내셔널]
**포지션명**: [백엔드 엔지니어]
**신입/경력**: [신입직]

**주요업무**:
- Node.js와 Express를 활용한 서버 개발
- MongoDB를 이용한 데이터 스토리지 설계 및 운영
- Redis를 활용한 캐시 및 세션 관리
- CI/CD 환경에서 Docker를 사용한 애플리케이션 배포 지원

**자격요건**:
- Node.js 및 Express 기본 사용 경험
- NoSQL 데이터베이스(MongoDB) 이해
- 자료구조 및 알고리즘의 기본적인 이해
- Git을 활용한 코드 관리 경험
'''

# CASE 2: 실제 데이터 (이준행 - OCR 텍스트 일부 요약 및 주요 내용 포함)
# (실제 데이터는 매우 길기 때문에 변수에 할당하여 처리)
case2_resume = '''

--- Page 1 ---
25 11 26
오후
12:06
공뇨
개발자
오빛나리
교육
0
Iol라e
채용공고
올리기
오빛나리님을
i8IY래위공응
#
이직/구직
중이에요
#
분석적
#
직관적
#커뮤니케이션
긍정적
#
꿍뇨
개발자
오빛나리
solitarybizegmailcom
유요
논리적
사고를
결합하여
문제를
해결하고
소통을
H우울
극우야
콩뇨
개발자
기술
스택
Java
React
Spring
Spring
Spring
JavaScript
Boot
Security
SQL
jQuery
HTML/CSS
AWS
경력
프리랜서
작가
작가
Yle표
작가
201707
재직중
Y리2피1룡98
공저
IoYIIo
출간
독립문예지/잡지/신문
워어
Io
칼럼
등
I이YI
등
기고
'ToYI1o
물른
릉을
집필
특정한
주제에
기衆短
북토크
글쓰기
|62
등
워
말하기/글쓰기
수행
프로젝트
추천해요
제안하기
커뮤니티
<혼;인:
덧글
0
자취생
https:/wwwrallitcom/hub/resumes/1408698/오빛나리
1/6

--- Page 2 ---
25. 11. 26. 2% 12:06
신입 백 엔
드 개발자
- 오
빚 나리 프로필
하 이 미디어 아카데미 자바 웹 개 발
자 교육 과정
2024.08. - 2024.09. (2 개
월)
프로젝트 개요
| 반 정보 공유, 동네 맛집 조회 및 등록, 중고 거래 등 다양한 기 능 을 제
공 하는 통합 커 듀 니티
자 취 하는 2030 청년층
사용 기술
o
백 엔 드 : Spring Boot, Spring Security, Java 17, JPA, MySQL
o
프 론 트 엔드: 8630, JavaScript, CSS3, Axios, Redux, Recoil
¢
API: Kakao Map API
* 데이터베이스: MySQL
o MAJZHEAL: WebSocket, STOMP
* 이미지 처리: AWS 53
o
배포: AWS 602
o 보안: JWT 인증 및 권한 관리
주요 기능
*
커뮤니티 게시판: 게시물 및 댓글 02, 인기 게시글 정렬
o 맛집 정보 공유: 625 기반 맛집 검색 및 리뷰 저장
* 취업 정 보 / 청 년 정책: 최신 정보 제공, 관리자 페 이 지 에서 업데이트 관리
o
중 고
거 래 게시판: AWS 53 이용한 다중 이미지 업로드, WebSocket 이용한 실시간 1:1 채팅
e
RESTful APl 872 확장
ㅎ WebSocket 7|2t 실시간 채팅 최적화
o JWTQIE 보안 적용
o AWS 기반 클라우드 배포
0%
o
후
o
팀 장 으로서 프로젝트 총괄 및 일정 관리
¢
Spring Security 기반 로그인 및 권한 관리 구현
* 다중 이미지 업로드 및 사진 업로드 포함한 실시간 1:1 채팅 기능 개발
* 관리자 페이지 및 중 고 거 -
댓글 O
htps://www.rallit.com/hub/resumes/1408698/2
L t2|

--- Page 3 ---
25. 11. 26. 오후 12:06
신입 백 엔 드
중
고 폰 직거래 웹 플랫폼 < 폰 테 일 >
하 이 미디어 아카데미 자바
웹 개발자 교육 과정
2024.05.- 2024.06. (2 개 월)
프로젝트 개요
중고 휴대폰 직거래 웹 플 랫 폼
을 설계 및 개 발 하여 상품 게시
시
스 템 을 구 축 했습니다.
백 엔 드 : Spring Boot, Java 17, JPA
o 프
론 트 엔드: HTML5, 0553
* 데이터베이스: MySQL
o 실시간 통신: WebSocket, STOMP
*
MH{: Apache Tomcat
주요 기능
* 상품 게시판: CRUD 기능, 상품 정보 저장
및 관리
'
o
1:1 채팅: WebSocketS 통한 실시간 채팅, 채
개발자 - 오
빚 나리 프로필
* 신고 기능: 관리자 페 이 지 에서 부적절한 게시물 및 회원 신고 처리
및
ㄷ 이노
움
후
e
0z
호
뿌
o
o
회원 관리: OAUth2.0 AN 2
성과
o 데이터 관리 최적화: .104^
와 Hibernate2 사용자 및 거래 데이터 관리
* 실시간 소통 강화: WebSocketS 통한 원활한 사용자 간 거래 및 소통 지원
팀 장 으
ㆍ 로그인 및 회원 관리 시스템 7 개발, 회원 상태 관리
보
아 4 및 다양한 기기 지원
스 토 리 텔링 시연
[ 웹 사이트 운용 시연: 사
용 ㅅ
htps://www.rallit.com/hub/resumes/1408698/2
L t2|
3/6

--- Page 4 ---
25 11 26
오후
12:06
콩뇨
개발자
오빛나리
프로젝트는
이
사용자경험을
중심으로
중고
휴대폰
거래
플랫폼을
시연했습니다
사용자는
중고
국매
구매를
위해로컬
극耳
OAuth
20를
활용한소설회원가입/로그인을
한후
WebSocket과 STOMP를
실시간1:1채팅을
통해
판매자와
소통하며거래를
진행했습니다
거래중문제가발생하면
신고기능을
통해
사용자를
신고할수있습니다
관리자는
신고내역을
실시간으로
더위누
블랙리스트등록회원차단
게시물
삭제
등의
조치를
신속히
처리할수있습니다
신고된
사용자는
이후거래제한을받으며
관리자는
효율적으로
문제를
해결할수
있도록
설계되었습니다
긍Y Io
신고후사용자관리라는
주요기능을
실시간채팅과
통해
사용자에게
안전한거래
릉병구
제공하는데중점을
두었으며
사용자와
관리자의
상호착용을
매끄럽게
극우로ㅎ
경험을
강조했습니다
포트폴리오
URL
6
깃허브프로필
?
프로젝트1<혼;인>구글슬라이드
6
프로젝트1<혼;인>
깃허브링크
?
른븐<最>海블丘
슬라이드
6
프로젝트2<폰테일>
깃허브링크
첨부파일
1
오빛나리
포트폴리오pdf
241024
자기소개
덧글
0
https:/wwwrallitcom/hub/resumes/1408698/오빛나리
4/6

--- Page 5 ---
25 11 26
오후
12:06
개발자
오빛나리
프로필
[영수증
애호가]
자바
웬개발자교육
과정을
통해
릉음건
Y둠해
영수증에
호기심과
탐구심이
생겼습니다
이전에는
단순한
종이로
보였던
영수증이
이제는
각종
데이터
처리와
|6물고릉
결과물로
를解
'o를
주문
번호와
고객
번호를
생성하는
오토
인크리먼트
상품의
수량과
가격을
계산하는
로직
세금과
총합을
출력하는
방식까지
영수증한
극어요
복잡한
음건
Io
숨어
있습니다
이제는
영수증을
머매룸
그과정이
|움0
상상하며
즐거움을느끼고
이는
실용적이면서도
기술적인
구현되었을지
문제해결의기쁨을
Yllo릉
발견하게
[도전과해결의
기쁨]
저는어려운문제를
때큰기쁨을
해결할
느낌니다
최종
프로젝트에서
Spring
Security와
 WebSocket
기능을주도한것도이도전
정신
실시간
통신이라는
복잡한
문제를
머우래우
코드의
흐름을
분석하고
최적화하는
과정은
마치
퍼츨을
푸는것같았습니다
Spring
Security를
통해
로그인
관리
문제를
Y殷버우룹우
시스템의
중요성을
실감했고
WebSocket을
H우융루
1:1채팅을
2y류
구축하며사용자경험을
즉각적으로
개선하는
성취감을
느꼈습니다
'내가
사용자라면어느
필요할까'
'내가사용자라면
기대하는
I어움요
1:1
채팅에서
|o곰ㅎ
뭘까'에대한
공리우
탐험이었습니다
그고민의
고민
과정도
결과로서
ID/PW
찾기기능과
이미지
전송가능한1:1채팅방기능을
구현했습니다
Io뮤러기]
Yolo오
[i8olo워
저는
항상
팀
프로젝트에서
팀장을
쇼를공물
소통을
통해
릉우
이끌어냈습니다
비록개발
|야무운
많지는
않았지만
우2
의지를
바탕으로
|를공물
신뢰를
얻었고
코드
리뷰
프로젝트
회고
규약논의
Ylo을
효율적인
릉우
주도할
수
있었습니다
작가로서
워어
사람들과
머우字
소통의
중요성을
체감한
경험은
개발
프로젝트에서도
큰도움이
되었고
|o를공물
워丘
의견을
공유하며
함께
성장할수
있는
조성했습니다
노션을
H우융륜
프로젝트
일정을
체계적으로
관리하고
H
프로젝트마다
회고를
통해도출된
다음
프로젝트에
반영했습니다
예를들어A미설계의
문제를
인식한후이후
프로젝트에서는
이를
개선한
RESTfuI API
설계를
위명국
경험이
있습니다
이러한과정을
통해
허를공물
소통과
릉야
더욱효과적으로
이끌어내며
리더십을
발휘했습니다그결과
Il아Y긍를공
"나리님이
Yo어오
릉뇨프극Mo워
주었고
이는
저에게큰동기부여가
되었습니다
[JAVA에서
살아남기]
저는
자바
웬개발자
교육
과정을
듣기
전까지
웹개발과는
비전공자였습니다
새로운
기술
IolFEIIY
릉류
들이는
것이
다소
두렵기도
했습니다
그러나
수업을
듣기
시작하면서
제가
오랫동안
해왔던
글쓰기와
개발이
여러
Yllo
유사하다는
것을
발견했습니다
논리적
인과관계
추정과결론도출
명료하고
깔끔한
코드
I뇨
'래
사용자
입장을
고려한
기능
설계
등에서그러한
점을느꼈습니다
교육
초반에는
미지의
세계를
극우무물
방랑자
같았지만
점차코딩의
매력을느끼고
업으로삼을수
있다는
o후
Io요롱 IY l무를
아무런
생각했던
영수증이이제는
'제게
하나의
작품처럼
보이듯이요
이제는
을습득하여
이를
기반으로
덧글
빠르게
H우로우
|o
실질적인
https:I/wwwrallitcom/hub/resumes/1408698/오빛나리

--- Page 6 ---
25. 11. 26. 2F 12:06
신입 백 엔
드 개발자 - 오 빛나리 프로필
교육
하 이 미디어 아카데미
사설 교육 | [ 실 무 프로젝트] 자바 웹 개발자 교육 과정
2024.03. - 2024.09. (졸업
명 지 대 학교
대 학 교 (학사) | 문 예 창 작 학과
송 삭
2012.03. - 2018.08. (졸업)
Powered by Rallit.
hitps://www.rallit.com/hub/resumes/1408698/2
L t2|

'''

case2_jd = '''
1 기업 서비스
1 / 3정리습관∙ 서울 강남구 ∙ 신입 이상
AI 리엑트 개발자
포지션 상세
정리습관은 ‘공간을 회복시키는 국내 최초 AI기반 공간 정리 플랫폼'입니다. 우리는 단순한 정리 서비스가 아닌, 고객의 라이프 스타일을 바꾸는 공간 컨설팅 플랫폼을 만듭니다. 2024년, 정리습관은 유수의 SEED 투자기관과 TIPS를 통해 연간 1500% 성장하며 제품과 시스템이 완성되었습니다. • 현재 고객 Web / 매니저 APP / 관리자 시스템 운영하고 있으며, 2026년 1분기 고객 APP 출시를 앞두고 있습니다. • 자체 상품 구조 및 AI 견적 시스템 개발 • 내부 대시보드와 Amplitude 기반 지표 관리 체계 구축 • 고객/매니저 데이터가 연결된 운영 자동화 구조 확립 이제 우리는 MVP 단계(SEED)를 넘어, 고객 검증을 완료하였고, 연간 매출 2026년 50억 -100억 원 성장 단계로 도약하고자 합니다. 현재 우리는 공간정리매니저 생태계 구축을 위해 매니저APP 활성화와 매니저 전문가 자격증 등 공간정리매니저 교육사업으로 확장하고 있으며, AI 기술을 활용한 공간컨설팅 챗봇 및 생활공간 이미지/영상 데이터 기반 디지털 트윈과 AR 기술화를 진행하고자 합니다. 그 여정을 함께 설계하고 나아갈 정리습관의 새로운 멤버, 개발자님을 기다리고 있습니다.
주요업무
• AI기반 고객용 웹 애플리케이션 개발 • AI agent 개발 • AI 기반 기능을 위한 UX/UI 설계 및 구현 • AI가 추천한 결과를 유저가 직관적으로 이해하고 선택할 수 있도록 정보 구조 설계 • 백엔드 AI 모듈과의 연결을 고려한 프론트 설계 및 로직 처리 • 서비스 성능 및 안정성 개선 • 프론트 최적화, 상태 관리 개선, 비동기 처리 성능 향상 • 문제 원인 빠르게 파악하고 실시간 해결하는 능동적 개선 • 운영/기획/디자인과 협업하여 지속적 개선
자격요건
• React Native (Expo) 기반 Cross-platform 앱 개발 경험 • React.js, Next.js 기반 웹 프론트엔드 개발 실무 경험 • TypeScript를 활용한 안정적이고 견고한 코드 작성 능력 • React Query 등 상태 관리 툴에 대한 이해 • Claude code 사용 능숙
기술 스택 • 툴
태그
마감일
상시채용
근무지역
1 서울 강남구 테헤란로 217, 3층 306호정리습관IT, 컨텐츠본 채용정보는 원티드랩의 동의없이 무단전재, 재배포, 재가공할 수 없으며, 구직활동 이외의 용도로 사용할 수 없습니다. 본 채용 정보는 에서 제공한 자료를 바탕으로 원티드랩에서 표현을 수정하고 이의 배열 및 구성을 편집하여 완성한 원티드랩의 저작자산이자 영업자산입니다. 본 정보 및 데이터베이스의 일부 내지는 전부에 대하여 원티드랩의 동의 없이 무단전재 또는 재배포, 재가공 및 크롤링할 수 없으며, 게재된 채용기업의 정보는 구직자의 구직활동 이외의 용도로 사용될 수 없습니다. 원티드랩은 에서 게재한 자료에 대한 오류나 그 밖에 원티드랩이 가공하지 않은 정보의 내용상 문제에 대하여 어떠한 보장도 하지 않으며, 사용자가 이를 신뢰하여 취한 조치에 대해 책임을 지지 않습니다. <저작권자 (주)원티드랩. 무단전재-재배포금지>아무리 찾아도 없던 포지션, 이제 에이전트로 바로 찾으세요!
# ==========================================
# 4. 실행 및 결과 출력
# ==========================================
'''

print("\n" + "="*50)
print("🧐 [CASE 1] 합성 데이터 분석 (신입 - 불일치 사례)")
print("="*50)
result1 = analyze_resume(case1_resume, case1_jd)
print(result1)

print("\n" + "="*50)
print("🧐 [CASE 2] 실제 데이터 분석 (시니어 - 고스펙 매칭 사례)")
print("="*50)
result2 = analyze_resume(case2_resume, case2_jd)
print(result2)

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer

# ==========================================
# 1. Base 모델 로드 (어댑터 제외)
# ==========================================
BASE_MODEL_ID = "Qwen/Qwen2.5-14B-Instruct"

print("🚀 Base 모델(Qwen-2.5-14B) 로드 중... (순수 모델)")

# 4-bit 양자화 설정 (메모리 절약)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

# Base 모델 로드
model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)
# ⚠️ 중요: PeftModel.from_pretrained() 부분을 삭제했습니다.
# 이제 이 model은 학습되지 않은 순정 상태입니다.

# 토크나이저 로드
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

# 스트리머 설정
streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

print("✅ Base 모델 로드 완료!")

# ==========================================
# 2. 추론 함수 (동일 로직)
# ==========================================
def analyze_resume_base(resume_text, jd_text):
    # 프롬프트는 Fine-tuning 때와 100% 동일하게 유지해야 '공정한 비교'가 됩니다.
    system_prompt = (
        "당신은 전문 테크니컬 리크루터이자 커리어 코치입니다.\n"
        "지원자의 이력서를 분석하여 보유 기술(Extracted Skills)을 파악하고, "
        "직무 기술서(JD)와의 격차(Gap)를 분석하는 것이 당신의 임무입니다.\n"
        "부족한 기술(Missing Skills)을 식별하고, 왜 그것이 부족한지 상세한 이유를 한국어로 설명하십시오."
        "missing_skills_chapters를 포함하여 JSON을 생성하라"
    )

    user_prompt = f"""
### Resume:
{resume_text}

### Job Description:
{jd_text}

Please analyze the resume skills and perform a Gap Analysis.
"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    model_inputs = tokenizer([text], return_tensors="pt").to("cuda")

    print("\n" + "="*30)
    print("🤖 Base Model 분석 시작 (Before 결과)")
    print("="*30 + "\n")

    with torch.no_grad():
        generated_ids = model.generate(
            **model_inputs,
            streamer=streamer,
            max_new_tokens=4096,
            temperature=0.1,
            do_sample=True,
            top_p=0.9,
            repetition_penalty=1.05
        )

# ==========================================
# 3. 데이터셋 (CASE 1 김수진 추천)
# ==========================================
# 슬라이드 비교용으로는 짧고 명확한 [CASE 1]이 더 좋습니다.
case1_resume = '''
- 이름: 김수진
- 생년월일 (만 26세)
- 전화번호: 010-****-1234
- 이메일: kimsoo123@xxmail.com

**학력:**
- OO초등학교 졸업
- OO중학교 졸업
- OO고등학교 졸업
- XX대학교 경영학과 (졸업)

**경력 및 경험:**
- 2022.03 - 2022.12: A문화센터, IT 지원 인턴
  - 외부 서버 관리 및 유지보수 지원
  - 기본적인 컴퓨터 문제 해결 및 사용자 지원 경험

**기술 및 역량:**
- Node.js 및 Express 사용 경험 없음
- MongoDB에 대한 기본적인 정보만 이해
- 데이터 구조 및 알고리즘에 대한 이해 부족
- Git 사용 경험 있으나 실제 프로젝트에 활용한 경험 없음

**기타:**
- 정보처리 기능사 자격증 취득
- IT 관련 비영리 단체에서 봉사활동 경험 (웹사이트 잘못된 링크 수정 등)
- 관련 프로젝트 경험 없음
'''

case1_jd = '''
**기업명**: [OO인터내셔널]
**포지션명**: [백엔드 엔지니어]
**신입/경력**: [신입직]

**주요업무**:
- Node.js와 Express를 활용한 서버 개발
- MongoDB를 이용한 데이터 스토리지 설계 및 운영
- Redis를 활용한 캐시 및 세션 관리
- CI/CD 환경에서 Docker를 사용한 애플리케이션 배포 지원

**자격요건**:
- Node.js 및 Express 기본 사용 경험
- NoSQL 데이터베이스(MongoDB) 이해
- 자료구조 및 알고리즘의 기본적인 이해
- Git을 활용한 코드 관리 경험
'''

print("\n" + "="*50)
print("📸 [캡처 준비] Base Model의 결과가 출력됩니다.")
print("="*50)

analyze_resume_base(case1_resume, case1_jd)

import torch
import warnings
warnings.filterwarnings("ignore") # Ignore warning messages

# 1. 라이브러리 설치
# 2. 호환성 수정된 버전 설치 (핵심: bitsandbytes를 0.45.0 이상으로 올림)
# Python 3.12 및 최신 Triton 지원은 0.45.0 버전부터 안정화되었습니다.
!pip install -q -U "bitsandbytes>=0.45.0"
!pip install -q -U git+https://github.com/huggingface/transformers.git
!pip install -q -U peft accelerate trl datasets

# 3. Triton 재설치 (최신 BnB와 호환되는 버전)
!pip install -q -U triton

print("재설치 완료. 상단 메뉴 [런타임] -> [세션 다시 시작] 후 import를 진행하세요.")
!pip install -q -U unsloth gradio

import torch
import bitsandbytes as bnb

# GPU 인식 확인
print(f"CUDA Available: {torch.cuda.is_available()}")
print(f"Bitsandbytes Version: {bnb.__version__}")

# 여기서 에러가 안 나야 모델 로드가 가능합니다.
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 1. 라이브러리 임포트
from unsloth import FastLanguageModel
from peft import PeftModel
from transformers import AutoTokenizer  # 추가됨: 원본 토크나이저 로드용
import torch
import os

# 2. 경로 설정
BASE_MODEL_ID = "Qwen/Qwen2.5-14B-Instruct"
ADAPTER_PATH = "/content/drive/MyDrive/Qwen2.5-14B-Resume-Analysis-Checkpoints/final_adapter"
SAVE_PATH = "./merged_qwen_14b" # 저장할 경로

# 3. 베이스 모델 로드 (Unsloth 활용)
print("🚀 베이스 모델 로드 중...")
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = BASE_MODEL_ID,
    max_seq_length = 4096,
    dtype = torch.bfloat16,
    load_in_4bit = True,
)

# 4. 저장된 LoRA 어댑터 로드
print("🔄 학습된 어댑터 로드 중...")
model = PeftModel.from_pretrained(model, ADAPTER_PATH)

# 5. 병합 및 저장 (vLLM용 포맷)
print("💾 모델 병합(Merge) 및 저장 중... (약 5~10분 소요)")

# 4bit QLoRA를 bfloat16으로 병합하여 vLLM에서 최고 속도를 내도록 변환
model = model.merge_and_unload()

# 모델 가중치 저장
model.save_pretrained(SAVE_PATH, safe_serialization=True)

# [핵심 수정 사항] 토크나이저를 원본(HuggingFace)에서 다시 받아 저장
# Unsloth 객체에 있는 tokenizer 대신, 원본 설정을 그대로 가져와야 vLLM 에러가 안 납니다.
print("🧩 토크나이저 설정 파일(config) 정합성 확보 중...")
original_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)
original_tokenizer.save_pretrained(SAVE_PATH)

print(f"✅ 모델 병합 및 토크나이저 저장 완료! 경로: {SAVE_PATH}")
print("👉 이제 [런타임] -> [세션 다시 시작]을 누르고 vLLM 서버를 실행하세요.")

import os
import subprocess
import sys
import time
from google.colab import drive

# 1. 구글 드라이브 연결
if not os.path.exists('/content/drive'):
    drive.mount('/content/drive')

# 2. 필수 라이브러리 설치
print("📦 라이브러리 확인 중...", flush=True)
subprocess.run([sys.executable, "-m", "pip", "install", "-q", "vllm", "openai", "gradio", "nest_asyncio", "autoawq"], check=True)

import gradio as gr
from openai import OpenAI
import nest_asyncio

nest_asyncio.apply()

# ==========================================
# 3. vLLM 서버 설정 및 실행
# ==========================================
ADAPTER_PATH = "/content/drive/MyDrive/Qwen2.5-14B-Resume-Analysis-Checkpoints/final_adapter"
BASE_MODEL_ID = "Qwen/Qwen2.5-14B-Instruct-AWQ"

print(f"\n🚀 vLLM 서버 시작 (4-bit AWQ + LoRA)", flush=True)

with open("vllm_server.log", "w") as f:
    f.write("--- Server Start ---\n")

# 서버 실행
vllm_process = subprocess.Popen(
    [
        sys.executable, "-m", "vllm.entrypoints.openai.api_server",
        "--model", BASE_MODEL_ID,
        "--quantization", "awq",
        "--dtype", "half",
        "--enable-lora",
        "--lora-modules", f"resume-analyzer={ADAPTER_PATH}",
        "--max-model-len", "16384",
        "--gpu-memory-utilization", "0.90",
        "--port", "8000",
        "--trust-remote-code",
        "--disable-log-stats"
    ],
    stdout=open("vllm_server.log", "a"),
    stderr=subprocess.STDOUT
)

# ==========================================
# 4. 실시간 로그 모니터링
# ==========================================
print("📜 서버 부팅 대기 중...", flush=True)
server_ready = False
start_time = time.time()

try:
    with open("vllm_server.log", "r") as f:
        while True:
            line = f.readline()
            if not line:
                time.sleep(0.1)
                if time.time() - start_time > 600:
                    print("\n❌ 타임아웃")
                    vllm_process.terminate()
                    break
                continue

            print(f"[vLLM] {line.strip()}", flush=True)

            if "Uvicorn running on http://0.0.0.0:8000" in line or "Application startup complete" in line:
                server_ready = True
                print("\n🎉 서버 준비 완료! Gradio를 시작합니다.\n", flush=True)
                break

            if "Traceback" in line or "Error" in line:
                if "HandlerFileNotFoundError" in line or "SageMaker" in line: continue
                if "Connection" not in line:
                    print("\n❌ 에러 발생")
                    vllm_process.terminate()
                    break

except KeyboardInterrupt:
    vllm_process.terminate()

# ==========================================
# 5. Gradio 인터페이스 (동시성 제한 해제)
# ==========================================
if server_ready:
    client = OpenAI(base_url="http://localhost:8000/v1", api_key="vllm")

    def predict(message, history):
        system_prompt = (
            "당신은 전문 테크니컬 리크루터이자 커리어 코치입니다.\n"
            "지원자의 이력서를 분석하여 보유 기술(Extracted Skills)을 파악하고, "
            "직무 기술서(JD)와의 격차(Gap)를 분석하는 것이 당신의 임무입니다.\n"
            "부족한 기술(Missing Skills)을 식별하고, 왜 그것이 부족한지 상세한 이유를 한국어로 설명하십시오."
            "반드시 **missing_skills_chapters** 키를 포함한 JSON 형식을 출력에 포함하십시오."
        )

        messages = [{"role": "system", "content": system_prompt}]
        for human, assistant in history:
            messages.append({"role": "user", "content": human})
            messages.append({"role": "assistant", "content": assistant})
        messages.append({"role": "user", "content": message})

        try:
            # [수정됨] stream=False로 설정 (안정적인 전송을 위해)
            response = client.chat.completions.create(
                model="resume-analyzer",
                messages=messages,
                temperature=0.1,
                max_tokens=4096,
                stream=False
            )
            # [수정됨] 반복문(for) 삭제 -> 한 번에 결과 반환(return)
            # 스트리밍이 아니므로 delta가 아니라 message.content에 전체 답변이 들어있습니다.
            return response.choices[0].message.content

        except Exception as e:
            return f"Error: {str(e)}"

    demo = gr.ChatInterface(
        fn=predict,
        title="AI 채용 분석 (4-bit AWQ + LoRA)",
        description="30명 동시 접속 가능 / vLLM 엔진 가동 중",
        chatbot=gr.Chatbot(height=500),
    )

    # [핵심 수정] 줄 세우기 해제: 한 번에 40명까지 통과시킴
    print("🔓 Gradio 동시 접속 제한을 해제합니다 (Limit: 40)", flush=True)
    demo.queue(default_concurrency_limit=40)

    demo.launch(share=True, debug=True)

# 1. 필요한 라이브러리 임포트
import torch
import gradio as gr
from unsloth import FastLanguageModel
from peft import PeftModel

# 2. 모델 및 어댑터 경로 설정
BASE_MODEL_ID = "Qwen/Qwen2.5-14B-Instruct"
ADAPTER_PATH = "/content/drive/MyDrive/Qwen2.5-14B-Resume-Analysis-Checkpoints/final_adapter"

# 3. 모델 로드 및 LoRA 어댑터 병합
print("🚀 모델 로드 중... (잠시만 기다려주세요)")

# 3-1. 베이스 모델 로드
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = BASE_MODEL_ID,
    max_seq_length = 4096,
    dtype = torch.bfloat16,  # A100 최적화
    load_in_4bit = True,
)

# 3-2. 저장된 LoRA 어댑터 로드 및 병합 (수정된 핵심 부분)
# 기존의 get_peft_model은 학습용이므로 제거하고, 저장된 가중치를 바로 불러옵니다.
print("🔄 어댑터 로드 및 병합 중...")
model = PeftModel.from_pretrained(model, ADAPTER_PATH)
model = model.merge_and_unload() # 추론 속도 향상을 위해 가중치 병합

# 3-3. 토크나이저 설정
# 패딩 토큰이 없는 경우 EOS 토큰으로 설정
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

print("✅ 모델 로드 및 어댑터 병합 완료!")

# 4. 추론 함수 정의
def predict(message, history):
    system_prompt = (
        "당신은 전문 테크니컬 리크루터이자 커리어 코치입니다.\n"
        "지원자의 이력서를 분석하여 보유 기술(Extracted Skills)을 파악하고, "
        "직무 기술서(JD)와의 격차(Gap)를 분석하는 것이 당신의 임무입니다.\n"
        "부족한 기술(Missing Skills)을 식별하고, 왜 그것이 부족한지 상세한 이유를 한국어로 설명하십시오."
        "missing_skills_chapters를 포함하여 JSON을 생성하라"
    )

    qwen_messages = []
    qwen_messages.append({"role": "system", "content": system_prompt})

    for human, assistant in history:
        qwen_messages.append({"role": "user", "content": human})
        qwen_messages.append({"role": "assistant", "content": assistant})

    qwen_messages.append({"role": "user", "content": message})

    prompt = tokenizer.apply_chat_template(
        qwen_messages,
        tokenize=False,
        add_generation_prompt=True
    )

    model_inputs = tokenizer([prompt], return_tensors="pt").to("cuda")

    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=4096,
        temperature=0.1,
        do_sample=True,
        top_p=0.9,
        repetition_penalty=1.05,
    )

    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

    return response

# 5. Gradio 실행 (호환성 최적화 버전)
print("🌍 Gradio ChatInterface를 시작합니다.")

demo = gr.ChatInterface(
    fn=predict,
    title="AI 채용 분석 및 역량 강화 챗봇 (Qwen2.5-14B-Instruct Fine-tuned)",
    description="이력서와 JD를 입력하면 AI가 보유 기술을 분석하고, JD와의 격차(Gap)를 상세히 설명해줍니다.",
    examples=[['''### Resume:\n의 교육 O 커리어 채 용 공고 올리기
<« 정 예 림 님 을 응 원 해보세요!
# 좋 은 제 안에 열 려 있어요 성실함 H 책임감 H 협 업 지향 H 유 연 함 # 커뮤니케이션
정 예 림
프 론 트 엔드 개발자
이메일 jinju151002@gmail.com
JHiiRieh 원 혈 한 ABE 위해 412 코 드 를 작 성 하기 위해 노 력 합니다. 과정 5 겪은 UIST ZEE 201 작 성 하고
있습니다.
고 명확한 설 명 으로 로 직 을 전 달 하여 다른 직군 사 람 들 과 대 화 하는 것을 좋 아 합니다.
사 용 성 을 고려한 디자인 시스템, 차 트 를 개 발 한 경 험 이 있습니다. 트 특징
스템,
징 에 따라 SSR,CSR 개발 환 경 을 설정, 개발, 배 포 까지 해본 경 험 이 있습니다.
기술 스택
React, next.js13, 5355, JavaScript, TypeScript, HTML/CSS, AWS, CI/CD, MobX, React Context,
storybook
24 =]
FAlZ|ALoto|x|ofloj A
팀원 | 솔루션 개발팀 | 재직 중
2021.07. ~ 재직 중 (4 년 5 개 월)
종합 데이터 테크 5355 기업인 010 X|0l|0] A0 DMP 기반 9 분석 서비스 모 바 일 인 덱 스 에서 프 론 트 엔드
개 발 자 로 근 무 하고 있습니다.
컴포넌트, 차트 , 개 발 블로그 등 추 천 해요 [5 제 안 하기 덧글 O GAME 개 발 에 참여
프로젝트
모 바 일 인덱스 GAME
Oto|xjof|of A
2023.09. ~ 2024.02.
o 모 바 일 게임 종합 인 텔 리 전 스 솔 루 션 인 모 바 일 인덱스 GAME 런 칭 에 참 여 하였습니다.
o 54 개 의 세 분 화 된 게임 태 그 별로 분 석 데 이 터 를 보여주는 서 비 스 로 다양한 데이터 시각화 컴 포 넌 트 를
개 발 했습니다.
o 데이터 조 회 에 필요한 날짜, 기 간 을 설 정 하는 08*6 민 야 < 안 와 RangePickerS 개 발 했습니다.
params 값 과 subRouteE 설 계 하여 메 인 페이지 위에 게임 상세, 퍼 플 리셔 상세 페 이 지 를 띄 웠습니다.
o 고 객 센터, 게임 태그 S form 기 능 과 검 색 기 능이 들어간 페 이 지 를 개 발 했습니다.
마케팅 클라우드
Oto|x|of|o] A 2022.12. ~ 2023.01.
o 별 도 의 시스템 구축 없이 마케팅 플 랫 폼 과 실시간 데 이 터 를 자유롭게 활 용 하는 원 스 톱 디지털 전환 마케팅 클라우드 개발 참여.
o 양 수 의 데 이 터 만 표현 가 능 하 던 8+ 차 트 에 음수 표현 기능 추가. o 개 인 정 보 처 리 방침 개 정 에 따른 이 용 약관 재 동 의 프로세스 개발.
¢ SEOE 위해 동적 56013. 적용
팀 개발 블로그 개발
oto|x|oflo A
2023.03. ~ 2023.07.
* ㅎ 16200/01(65 데이터 사 업 부 분 솔 루 션 개 발 팀 의
o 관 심 사 에 맞는 글 을 빠르게 찾 기 위한 태그 필터 기 o 메인 페이지 블 로 글 목 록 에 Infinite Scroll 적용.
* 쿠 키 (000<6) 를 활 용 하여 비 로 그 인 유 저 도 좋아요 누를 수 있는 기능. * ㅎ dynamic import®t lazyS 사 용 하여 코드 번들 사이즈 감소.
* docker-compose®} docker-networks 사 용 하여 이 /00 구축
I N 록 름 Hu 1
o N Jot
o 쿠 [ 으
o 뜨 릉 해 우 Ed 고 og
39
따 기 £
모 바 일 인덱스 브랜드 가이드
Oto|x|ofjof2i2
2023.08. ~ 2023.08.
데스크 탑 , 780[>×※, 580px, 360px 4 가 지 기 준 으로 반 응 형 대 음 .
06018 쿼리, grid, aspect-ratio, 5355 의 함수 기능
네비게이션 자동 스크롤, 다 크 모드, 클 립 보드 등 사 용 자 외
605 배포 경험 및 학습
560 최 적 화 를 위해 MetaEll 12} sitemap 제출
기 0 2 4
_ 9
i" ,
fo r
= 호 수 융
포트폴리오
URL
포트폴리오
3| 으 교육
삼성 청년 SW OBHEIO| (SSAFY) s
2020. 07 - 2021.07. | 졸업
금 오 공 과 대 학교
대 학 교 ( 전 문 학사) | 산 업 경 영 공 학 전공
2015.03. ~ 2020.02. | 졸업
Powered by Rallit.\n\n### Job Description:\n0 이력서
1 교육•이벤트
2 콘텐츠
3 소셜
4 프리랜서
1 기업 서비스
1 / 9당근마켓∙ 서울 서초구 ∙ 경력 1-5년
Data Analytics Engineer - Local Business
포지션 상세
[Local Business를 소개해요] Local Business는 동네 이웃과 동네 업체의 활발한 연결을 만들어내는 일을 하고 있어요. 당근 사용자들이 동네 가게의 다양한 정보와 소식을 쉽고 즐겁게 발견할 수 있도록 경험을 만들고, 사장님들에게는 단골 고객과 장기적으로 소통하며 관계를 쌓을 수 있는 도구를 제공해요. 나아가 동네 안에서 가게와 이웃이 신뢰와 가치를 주고받는 지속 가능한 로컬 상권 생태계를 만들어가고 있어요. [Data Analytics Engineer를 소개해요] Data Analytics Engineer는 데이터가 신뢰할 수 있고 일관되게 활용되며, 실제 비즈니스와 사용자에게 가치를 제공할 수 있도록 데이터 모델링, 엔지니어링, 분석을 넘나들며 일해요. 당근의 다양한 서비스와 규모 있는 데이터 환경 속에서 분석가, 엔지니어, 제품팀이 데이터를 안정적으로 활용할 수 있도록 데이터의 수집부터 활용까지 전반적인 흐름을 주도적으로 설계하고 개선해요. 또한 데이터가 단순히 쌓이는 것을 넘어, 데이터로부터 빠르고 정확하게 인사이트를 얻고 좋은 의사결정을 할 수 있도록 데이터 마트를 설계하고 품질을 관리하며, 데이터 거버넌스 체계를 운영하고 필요에 따라 기본적인 분석이나 실험을 수행해 데이터 기반 의사결정을 지원해요.
주요업무
• 로그 및 이벤트 데이터의 정의, 설계, QA, 모니터링 전 과정을 관리해요 • 로그 단의 데이터를 구성원들이 신뢰하고 활용도 높게 사용할 수 있는 마트 데이터로 만들어요 • 데이터 모델링과 품질 관리를 통해 신뢰도 높은 데이터 환경을 구축하고, 운영 중 관측성(Observability)과 자동화된 품질 체크를 통해 문제를 조기에 발견하고 해결해요 • 데이터 거버넌스 체계 운영과 문서화를 통해 조직 전체의 데이터 신뢰성을 높여요 • BI 툴과 데이터 카탈로그를 활용해 구성원들이 데이터를 쉽게 탐색하고 이해할 수 있는 환경을 만들어요
자격요건
• 데이터 신뢰성, 품질, 거버넌스를 고민하고 지속 관리한 경험이 있으신 분 • 조직의 비즈니스 현황과 목표에 대한 이해를 바탕으로 구성원 활용도가 높은 마트 테이블을 서빙하실 수 있는 분 • 데이터 모델링/ETL/데이터 웨어하우스 구축 경험이 있으신 분 • 단일 모델 수준이 아닌, 데이터 파이프라인 전반의 데이터 품질 관리, 정합성 검증 경험이 있으신 분 • SQL, Python을 활용해 데이터를 분석하고 처리할 수 있으신 분 • 다양한 직군(분석가, PM, 엔지니어)과 협업 및 커뮤니케이션으로 효율적인 데이터 구조를 설계해본 분
기술 스택 • 툴
태그
마감일
상시채용
근무지역
1 서울 서초구 강남대로 465, 강남 교보타워 7, 10, 11, 12층당근마켓IT, 컨텐츠본 채용정보는 원티드랩의 동의없이 무단전재, 재배포, 재가공할 수 없으며, 구직활동 이외의 용도로 사용할 수 없습니다. 본 채용 정보는 에서 제공한 자료를 바탕으로 원티드랩에서 표현을 수정하고 이의 배열 및 구성을 편집하여 완성한 원티드랩의 저작자산이자 영업자산입니다. 본 정보 및 데이터베이스의 일부 내지는 전부에 대하여 원티드랩의 동의 없이 무단전재 또는 재배포, 재가공 및 크롤링할 수 없으며, 게재된 채용기업의 정보는 구직자의 구직활동 이외의 용도로 사용될 수 없습니다. 원티드랩은 에서 게재한 자료에 대한 오류나 그 밖에 원티드랩이 가공하지 않은 정보의 내용상 문제에 대하여 어떠한 보장도 하지 않으며, 사용자가 이를 신뢰하여 취한 조치에 대해 책임을 지지 않습니다. <저작권자 (주)원티드랩. 무단전재-재배포금지>아무리 찾아도 없던 포지션, 이제 에이전트로 바로 찾으세요!
\n\nPlease analyze the resume skills and perform a Gap Analysis.''']],
    chatbot=gr.Chatbot(height=500),
    # 에러를 유발하는 디자인 옵션들(theme, submit_btn, clear_btn, autofocus)을 모두 제거했습니다.
)

demo.launch(share=True, debug=True)

# ==========================================
# [CASE 3] 완전 새로운 데이터 (Unseen Data - Java 백엔드)
# ==========================================

case3_resume = '''
의 교육 O 커리어 채 용 공고 올리기
<« 정 예 림 님 을 응 원 해보세요!
# 좋 은 제 안에 열 려 있어요 성실함 H 책임감 H 협 업 지향 H 유 연 함 # 커뮤니케이션
정 예 림
프 론 트 엔드 개발자
이메일 jinju151002@gmail.com
JHiiRieh 원 혈 한 ABE 위해 412 코 드 를 작 성 하기 위해 노 력 합니다. 과정 5 겪은 UIST ZEE 201 작 성 하고
있습니다.
고 명확한 설 명 으로 로 직 을 전 달 하여 다른 직군 사 람 들 과 대 화 하는 것을 좋 아 합니다.
사 용 성 을 고려한 디자인 시스템, 차 트 를 개 발 한 경 험 이 있습니다. 트 특징
스템,
징 에 따라 SSR,CSR 개발 환 경 을 설정, 개발, 배 포 까지 해본 경 험 이 있습니다.
기술 스택
React, next.js13, 5355, JavaScript, TypeScript, HTML/CSS, AWS, CI/CD, MobX, React Context,
storybook
24 =]
FAlZ|ALoto|x|ofloj A
팀원 | 솔루션 개발팀 | 재직 중
2021.07. ~ 재직 중 (4 년 5 개 월)
종합 데이터 테크 5355 기업인 010 X|0l|0] A0 DMP 기반 9 분석 서비스 모 바 일 인 덱 스 에서 프 론 트 엔드
개 발 자 로 근 무 하고 있습니다.
컴포넌트, 차트 , 개 발 블로그 등 추 천 해요 [5 제 안 하기 덧글 O GAME 개 발 에 참여
프로젝트
모 바 일 인덱스 GAME
Oto|xjof|of A
2023.09. ~ 2024.02.
o 모 바 일 게임 종합 인 텔 리 전 스 솔 루 션 인 모 바 일 인덱스 GAME 런 칭 에 참 여 하였습니다.
o 54 개 의 세 분 화 된 게임 태 그 별로 분 석 데 이 터 를 보여주는 서 비 스 로 다양한 데이터 시각화 컴 포 넌 트 를
개 발 했습니다.
o 데이터 조 회 에 필요한 날짜, 기 간 을 설 정 하는 08*6 민 야 < 안 와 RangePickerS 개 발 했습니다.
params 값 과 subRouteE 설 계 하여 메 인 페이지 위에 게임 상세, 퍼 플 리셔 상세 페 이 지 를 띄 웠습니다.
o 고 객 센터, 게임 태그 S form 기 능 과 검 색 기 능이 들어간 페 이 지 를 개 발 했습니다.
마케팅 클라우드
Oto|x|of|o] A 2022.12. ~ 2023.01.
o 별 도 의 시스템 구축 없이 마케팅 플 랫 폼 과 실시간 데 이 터 를 자유롭게 활 용 하는 원 스 톱 디지털 전환 마케팅 클라우드 개발 참여.
o 양 수 의 데 이 터 만 표현 가 능 하 던 8+ 차 트 에 음수 표현 기능 추가. o 개 인 정 보 처 리 방침 개 정 에 따른 이 용 약관 재 동 의 프로세스 개발.
¢ SEOE 위해 동적 56013. 적용
팀 개발 블로그 개발
oto|x|oflo A
2023.03. ~ 2023.07.
* ㅎ 16200/01(65 데이터 사 업 부 분 솔 루 션 개 발 팀 의
o 관 심 사 에 맞는 글 을 빠르게 찾 기 위한 태그 필터 기 o 메인 페이지 블 로 글 목 록 에 Infinite Scroll 적용.
* 쿠 키 (000<6) 를 활 용 하여 비 로 그 인 유 저 도 좋아요 누를 수 있는 기능. * ㅎ dynamic import®t lazyS 사 용 하여 코드 번들 사이즈 감소.
* docker-compose®} docker-networks 사 용 하여 이 /00 구축
I N 록 름 Hu 1
o N Jot
o 쿠 [ 으
o 뜨 릉 해 우 Ed 고 og
39
따 기 £
모 바 일 인덱스 브랜드 가이드
Oto|x|ofjof2i2
2023.08. ~ 2023.08.
데스크 탑 , 780[>×※, 580px, 360px 4 가 지 기 준 으로 반 응 형 대 음 .
06018 쿼리, grid, aspect-ratio, 5355 의 함수 기능
네비게이션 자동 스크롤, 다 크 모드, 클 립 보드 등 사 용 자 외
605 배포 경험 및 학습
560 최 적 화 를 위해 MetaEll 12} sitemap 제출
기 0 2 4
_ 9
i" ,
fo r
= 호 수 융
포트폴리오
URL
포트폴리오
3| 으 교육
삼성 청년 SW OBHEIO| (SSAFY) s
2020. 07 - 2021.07. | 졸업
금 오 공 과 대 학교
대 학 교 ( 전 문 학사) | 산 업 경 영 공 학 전공
2015.03. ~ 2020.02. | 졸업
Powered by Rallit.
'''

case3_jd = '''
0 이력서
1 교육•이벤트
2 콘텐츠
3 소셜
4 프리랜서
1 기업 서비스
1 / 9당근마켓∙ 서울 서초구 ∙ 경력 1-5년
Data Analytics Engineer - Local Business
포지션 상세
[Local Business를 소개해요] Local Business는 동네 이웃과 동네 업체의 활발한 연결을 만들어내는 일을 하고 있어요. 당근 사용자들이 동네 가게의 다양한 정보와 소식을 쉽고 즐겁게 발견할 수 있도록 경험을 만들고, 사장님들에게는 단골 고객과 장기적으로 소통하며 관계를 쌓을 수 있는 도구를 제공해요. 나아가 동네 안에서 가게와 이웃이 신뢰와 가치를 주고받는 지속 가능한 로컬 상권 생태계를 만들어가고 있어요. [Data Analytics Engineer를 소개해요] Data Analytics Engineer는 데이터가 신뢰할 수 있고 일관되게 활용되며, 실제 비즈니스와 사용자에게 가치를 제공할 수 있도록 데이터 모델링, 엔지니어링, 분석을 넘나들며 일해요. 당근의 다양한 서비스와 규모 있는 데이터 환경 속에서 분석가, 엔지니어, 제품팀이 데이터를 안정적으로 활용할 수 있도록 데이터의 수집부터 활용까지 전반적인 흐름을 주도적으로 설계하고 개선해요. 또한 데이터가 단순히 쌓이는 것을 넘어, 데이터로부터 빠르고 정확하게 인사이트를 얻고 좋은 의사결정을 할 수 있도록 데이터 마트를 설계하고 품질을 관리하며, 데이터 거버넌스 체계를 운영하고 필요에 따라 기본적인 분석이나 실험을 수행해 데이터 기반 의사결정을 지원해요.
주요업무
• 로그 및 이벤트 데이터의 정의, 설계, QA, 모니터링 전 과정을 관리해요 • 로그 단의 데이터를 구성원들이 신뢰하고 활용도 높게 사용할 수 있는 마트 데이터로 만들어요 • 데이터 모델링과 품질 관리를 통해 신뢰도 높은 데이터 환경을 구축하고, 운영 중 관측성(Observability)과 자동화된 품질 체크를 통해 문제를 조기에 발견하고 해결해요 • 데이터 거버넌스 체계 운영과 문서화를 통해 조직 전체의 데이터 신뢰성을 높여요 • BI 툴과 데이터 카탈로그를 활용해 구성원들이 데이터를 쉽게 탐색하고 이해할 수 있는 환경을 만들어요
자격요건
• 데이터 신뢰성, 품질, 거버넌스를 고민하고 지속 관리한 경험이 있으신 분 • 조직의 비즈니스 현황과 목표에 대한 이해를 바탕으로 구성원 활용도가 높은 마트 테이블을 서빙하실 수 있는 분 • 데이터 모델링/ETL/데이터 웨어하우스 구축 경험이 있으신 분 • 단일 모델 수준이 아닌, 데이터 파이프라인 전반의 데이터 품질 관리, 정합성 검증 경험이 있으신 분 • SQL, Python을 활용해 데이터를 분석하고 처리할 수 있으신 분 • 다양한 직군(분석가, PM, 엔지니어)과 협업 및 커뮤니케이션으로 효율적인 데이터 구조를 설계해본 분
기술 스택 • 툴
태그
마감일
상시채용
근무지역
1 서울 서초구 강남대로 465, 강남 교보타워 7, 10, 11, 12층당근마켓IT, 컨텐츠본 채용정보는 원티드랩의 동의없이 무단전재, 재배포, 재가공할 수 없으며, 구직활동 이외의 용도로 사용할 수 없습니다. 본 채용 정보는 에서 제공한 자료를 바탕으로 원티드랩에서 표현을 수정하고 이의 배열 및 구성을 편집하여 완성한 원티드랩의 저작자산이자 영업자산입니다. 본 정보 및 데이터베이스의 일부 내지는 전부에 대하여 원티드랩의 동의 없이 무단전재 또는 재배포, 재가공 및 크롤링할 수 없으며, 게재된 채용기업의 정보는 구직자의 구직활동 이외의 용도로 사용될 수 없습니다. 원티드랩은 에서 게재한 자료에 대한 오류나 그 밖에 원티드랩이 가공하지 않은 정보의 내용상 문제에 대하여 어떠한 보장도 하지 않으며, 사용자가 이를 신뢰하여 취한 조치에 대해 책임을 지지 않습니다. <저작권자 (주)원티드랩. 무단전재-재배포금지>아무리 찾아도 없던 포지션, 이제 에이전트로 바로 찾으세요!
'''

# 실행 코드
print("\n" + "="*50)
print("🧐 [CASE 3] Unseen 데이터 분석 (진짜 실력 검증)")
print("="*50)
print(analyze_resume(case3_resume, case3_jd))